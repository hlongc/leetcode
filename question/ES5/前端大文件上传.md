# 前端大文件上传实现详解

## 一、大文件上传的基本原理

大文件上传的核心思想是将大文件分割成多个小块（chunks），然后分别上传这些小块，最后在服务器端将这些块重新组合成完整的文件。这种方法解决了以下问题：

1. **浏览器内存限制**：避免一次性加载大文件导致内存占用过高
2. **上传失败重试**：单个块上传失败只需重传该块，而不是整个文件
3. **并发控制**：可以控制并发上传的块数量，提高上传效率
4. **断点续传**：记录已上传的块，支持中断后继续上传
5. **秒传功能**：通过文件指纹判断服务器是否已存在相同文件

## 二、实现步骤

### 1. 文件分片（Chunk）

使用 Blob API 的 `slice` 方法将文件分割成固定大小的块：

```javascript
function createFileChunks(file, chunkSize = 2 * 1024 * 1024) {
  const chunks = [];
  let startPos = 0;

  while (startPos < file.size) {
    const endPos = Math.min(startPos + chunkSize, file.size);
    const chunk = file.slice(startPos, endPos);
    chunks.push({
      index: Math.floor(startPos / chunkSize),
      chunk,
      size: chunk.size,
      startPos,
      endPos,
    });
    startPos = endPos;
  }

  return chunks;
}
```

### 2. 文件唯一标识（Hash）计算

为了实现秒传和断点续传，需要给每个文件生成唯一的标识符：

```javascript
async function calculateFileHash(file) {
  // 方法一：使用整个文件内容计算hash（适合小文件）
  // return await hashFile(file);

  // 方法二：抽样哈希，取文件的部分内容计算（适合大文件）
  return await sampleHashFile(file);
}

async function sampleHashFile(file) {
  // 取文件的前、中、后各2M内容，再加上文件大小信息
  const sampleSize = 2 * 1024 * 1024; // 2MB
  const totalSamples = [];

  // 文件头部
  totalSamples.push(file.slice(0, sampleSize));

  // 文件中部
  const middleStart = Math.floor(file.size / 2) - Math.floor(sampleSize / 2);
  totalSamples.push(file.slice(middleStart, middleStart + sampleSize));

  // 文件尾部
  const endStart = file.size - sampleSize;
  totalSamples.push(file.slice(endStart > 0 ? endStart : 0));

  // 文件信息
  totalSamples.push(
    new Blob([`${file.name}-${file.size}-${file.lastModified}`])
  );

  // 使用工具库如SparkMD5或Web Crypto API计算哈希
  const spark = new SparkMD5.ArrayBuffer();
  for (const sample of totalSamples) {
    const buffer = await sample.arrayBuffer();
    spark.append(buffer);
  }

  return spark.end();
}
```

### 3. 分片上传实现

```javascript
async function uploadChunks(chunks, fileHash, filename, onProgress) {
  // 创建上传任务列表
  const uploadTasks = chunks.map((chunk, index) => {
    const formData = new FormData();
    formData.append("chunk", chunk.chunk);
    formData.append("hash", fileHash);
    formData.append("filename", filename);
    formData.append("chunkIndex", chunk.index);
    formData.append("totalChunks", chunks.length);

    return { formData, index, retry: 3 }; // 设置重试次数
  });

  // 并发控制
  const concurrentUpload = async (tasks, max = 4) => {
    let currentIndex = 0;
    let completedCount = 0;

    const executor = async () => {
      while (currentIndex < tasks.length) {
        const task = tasks[currentIndex++];
        try {
          await uploadChunk(task.formData);
          completedCount++;
          onProgress(completedCount / tasks.length);
        } catch (error) {
          if (task.retry > 0) {
            task.retry--;
            // 失败后重新加入队列
            tasks.push(task);
          } else {
            // 多次重试失败，可能需要终止上传
            throw new Error(
              `Chunk ${task.index} upload failed after multiple retries`
            );
          }
        }
      }
    };

    // 创建多个上传任务并发执行
    const executors = Array(Math.min(max, tasks.length)).fill(0).map(executor);

    await Promise.all(executors);
  };

  // 开始并发上传
  await concurrentUpload(uploadTasks);

  // 所有分片上传完成，通知服务器合并文件
  await mergeChunks(fileHash, filename, chunks.length);
}

async function uploadChunk(formData) {
  const response = await fetch("/api/upload/chunk", {
    method: "POST",
    body: formData,
  });

  if (!response.ok) {
    throw new Error("Chunk upload failed");
  }

  return await response.json();
}

async function mergeChunks(fileHash, filename, totalChunks) {
  const response = await fetch("/api/upload/merge", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      fileHash,
      filename,
      totalChunks,
    }),
  });

  if (!response.ok) {
    throw new Error("File merge failed");
  }

  return await response.json();
}
```

## 三、秒传功能实现

秒传的核心是在上传前检查服务器是否已存在相同的文件（通过文件 Hash 判断）：

```javascript
async function uploadFile(file, onProgress) {
  // 步骤1: 计算文件哈希
  const fileHash = await calculateFileHash(file);

  // 步骤2: 检查文件是否已存在（秒传验证）
  const verifyResponse = await fetch("/api/upload/verify", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      hash: fileHash,
      filename: file.name,
    }),
  });

  const verifyResult = await verifyResponse.json();

  // 文件已存在，直接提示上传成功（秒传）
  if (verifyResult.exists) {
    onProgress(1); // 进度直接设为100%
    return {
      url: verifyResult.url,
      fileHash,
    };
  }

  // 文件不存在或需要续传，继续后续步骤
  const chunks = createFileChunks(file);

  // 如果有未完成的分片记录，过滤掉已上传的部分
  if (verifyResult.uploadedChunks && verifyResult.uploadedChunks.length > 0) {
    const remainingChunks = chunks.filter(
      (chunk) => !verifyResult.uploadedChunks.includes(chunk.index)
    );

    // 断点续传：只上传未完成的分片
    await uploadChunks(remainingChunks, fileHash, file.name, (progress) => {
      // 计算实际进度，考虑已上传的部分
      const uploadedCount = verifyResult.uploadedChunks.length;
      const totalProgress =
        (uploadedCount + progress * remainingChunks.length) / chunks.length;
      onProgress(totalProgress);
    });
  } else {
    // 全新上传
    await uploadChunks(chunks, fileHash, file.name, onProgress);
  }

  return {
    url: `/file/${fileHash}`,
    fileHash,
  };
}
```

## 四、断点续传功能实现

断点续传依赖于服务器端存储每个文件已上传分片的记录，并在重新上传时跳过这些分片：

### 前端部分：

1. **记录上传状态**：在本地存储（如 IndexedDB 或 localStorage）中保存上传进度

```javascript
// 保存上传状态到本地
function saveUploadStatus(fileHash, uploadedChunks) {
  localStorage.setItem(
    `upload_${fileHash}`,
    JSON.stringify({
      uploadedChunks,
      timestamp: Date.now(),
    })
  );
}

// 获取本地上传状态
function getUploadStatus(fileHash) {
  const status = localStorage.getItem(`upload_${fileHash}`);
  if (!status) return null;

  try {
    return JSON.parse(status);
  } catch (e) {
    return null;
  }
}

// 更新分片上传状态
function updateChunkStatus(fileHash, chunkIndex) {
  const status = getUploadStatus(fileHash) || { uploadedChunks: [] };
  if (!status.uploadedChunks.includes(chunkIndex)) {
    status.uploadedChunks.push(chunkIndex);
  }
  saveUploadStatus(fileHash, status.uploadedChunks);
}
```

2. **恢复上传功能**：

```javascript
async function resumeUpload(file) {
  const fileHash = await calculateFileHash(file);
  const status = getUploadStatus(fileHash);

  if (!status || !status.uploadedChunks.length) {
    // 没有本地记录，从头开始上传
    return uploadFile(file, onProgress);
  }

  // 有本地记录，同时向服务器验证已上传的分片
  const verifyResponse = await fetch("/api/upload/verify", {
    method: "POST",
    headers: {
      "Content-Type": "application/json",
    },
    body: JSON.stringify({
      hash: fileHash,
      filename: file.name,
      uploadedChunks: status.uploadedChunks,
    }),
  });

  const verifyResult = await verifyResponse.json();

  if (verifyResult.exists) {
    // 文件已完全上传
    return {
      url: verifyResult.url,
      fileHash,
    };
  }

  // 继续断点续传
  const chunks = createFileChunks(file);
  const remainingChunks = chunks.filter(
    (chunk) => !verifyResult.uploadedChunks.includes(chunk.index)
  );

  await uploadChunks(remainingChunks, fileHash, file.name, (progress) => {
    const totalProgress =
      (verifyResult.uploadedChunks.length + progress * remainingChunks.length) /
      chunks.length;
    onProgress(totalProgress);
  });

  return {
    url: `/file/${fileHash}`,
    fileHash,
  };
}
```

### 服务器部分（伪代码）：

```javascript
// 验证文件状态
app.post("/api/upload/verify", (req, res) => {
  const { hash, filename, uploadedChunks = [] } = req.body;

  // 检查文件是否已经存在
  if (fs.existsSync(`${UPLOAD_DIR}/${hash}`)) {
    return res.json({
      exists: true,
      url: `/file/${hash}`,
    });
  }

  // 检查已上传的分片
  const chunksDir = `${UPLOAD_DIR}/${hash}-chunks`;
  let serverUploadedChunks = [];

  if (fs.existsSync(chunksDir)) {
    serverUploadedChunks = fs
      .readdirSync(chunksDir)
      .filter((name) => name.match(/^\d+$/))
      .map((name) => parseInt(name));
  }

  // 返回服务器确认的已上传分片列表
  return res.json({
    exists: false,
    uploadedChunks: serverUploadedChunks,
  });
});

// 合并文件分片
app.post("/api/upload/merge", (req, res) => {
  const { fileHash, filename, totalChunks } = req.body;
  const chunksDir = `${UPLOAD_DIR}/${fileHash}-chunks`;
  const filePath = `${UPLOAD_DIR}/${fileHash}`;

  // 检查所有分片是否都已上传
  const uploadedChunks = fs
    .readdirSync(chunksDir)
    .filter((name) => name.match(/^\d+$/))
    .map((name) => parseInt(name));

  if (uploadedChunks.length !== totalChunks) {
    return res.status(400).json({
      error: "Some chunks are missing",
    });
  }

  // 按序合并分片
  const writeStream = fs.createWriteStream(filePath);
  for (let i = 0; i < totalChunks; i++) {
    const chunkBuffer = fs.readFileSync(`${chunksDir}/${i}`);
    writeStream.write(chunkBuffer);
  }
  writeStream.end();

  // 合并完成后，可以选择删除分片目录
  // fs.rmdirSync(chunksDir, { recursive: true });

  res.json({
    url: `/file/${fileHash}`,
  });
});
```

## 五、实现中的难点与解决方案

### 1. 文件 Hash 计算性能问题

**难点**：对大文件计算 Hash 会阻塞主线程，导致界面卡顿

**解决方案**：

1. **Web Worker**：将 Hash 计算放到 Web Worker 中进行

```javascript
// main.js
function calculateHashInWorker(file) {
  return new Promise((resolve, reject) => {
    const worker = new Worker("/hash-worker.js");

    worker.onmessage = (e) => {
      if (e.data.error) {
        reject(e.data.error);
      } else {
        resolve(e.data.hash);
      }
      worker.terminate();
    };

    worker.onerror = (err) => {
      reject(err);
      worker.terminate();
    };

    worker.postMessage({ file });
  });
}

// hash-worker.js
importScripts("/spark-md5.min.js");

self.onmessage = async (e) => {
  const { file } = e.data;
  const spark = new SparkMD5.ArrayBuffer();
  const chunks = [];
  const chunkSize = 2 * 1024 * 1024;
  let startPos = 0;

  // 分片读取文件内容
  while (startPos < file.size) {
    const endPos = Math.min(startPos + chunkSize, file.size);
    const chunk = file.slice(startPos, endPos);
    chunks.push(chunk);
    startPos = endPos;
  }

  try {
    for (const chunk of chunks) {
      const buffer = await chunk.arrayBuffer();
      spark.append(buffer);

      // 向主线程报告进度
      self.postMessage({
        progress: startPos / file.size,
      });
    }

    self.postMessage({
      hash: spark.end(),
    });
  } catch (error) {
    self.postMessage({
      error: error.message,
    });
  }
};
```

2. **抽样 Hash**：不计算整个文件的 Hash，而是取文件的部分内容计算 Hash

### 2. 并发控制

**难点**：同时上传太多分片会占用大量网络资源，太少又会影响上传速度

**解决方案**：

1. **动态并发控制**：根据网络状况动态调整并发数

```javascript
class TaskController {
  constructor(maxConcurrent = 4) {
    this.queue = [];
    this.running = 0;
    this.maxConcurrent = maxConcurrent;
    this.results = [];
  }

  addTask(task) {
    return new Promise((resolve, reject) => {
      this.queue.push({
        task,
        resolve,
        reject,
      });
      this.runNext();
    });
  }

  runNext() {
    if (this.running >= this.maxConcurrent || this.queue.length === 0) {
      return;
    }

    this.running++;
    const { task, resolve, reject } = this.queue.shift();

    const networkSpeed = this.evaluateNetworkSpeed();

    // 动态调整并发数
    if (networkSpeed > 10) {
      // 速度快，增加并发
      this.maxConcurrent = Math.min(this.maxConcurrent + 2, 10);
    } else if (networkSpeed < 3) {
      // 速度慢，减少并发
      this.maxConcurrent = Math.max(this.maxConcurrent - 1, 2);
    }

    task()
      .then((result) => {
        this.running--;
        resolve(result);
        this.runNext();
      })
      .catch((err) => {
        this.running--;
        reject(err);
        this.runNext();
      });
  }

  evaluateNetworkSpeed() {
    // 实际应用中可以记录最近几次上传的速度来评估
    return 5; // 示例返回值
  }
}
```

### 3. 大文件切片效率问题

**难点**：处理超大文件时，创建过多的小文件切片会导致内存占用过高

**解决方案**：

1. **动态切片大小**：根据文件大小动态调整切片大小

```javascript
function calculateChunkSize(fileSize) {
  // 文件<20MB，使用2MB切片
  if (fileSize < 20 * 1024 * 1024) {
    return 2 * 1024 * 1024;
  }
  // 文件<100MB，使用5MB切片
  else if (fileSize < 100 * 1024 * 1024) {
    return 5 * 1024 * 1024;
  }
  // 文件<1GB，使用10MB切片
  else if (fileSize < 1024 * 1024 * 1024) {
    return 10 * 1024 * 1024;
  }
  // 文件>1GB，使用20MB切片
  else {
    return 20 * 1024 * 1024;
  }
}

// 创建切片时使用动态大小
function createFileChunks(file) {
  const chunkSize = calculateChunkSize(file.size);
  // 后续代码同上...
}
```

2. **逐步切片**：不一次性创建所有切片，而是在上传过程中逐步创建

```javascript
async function uploadLargeFile(file, onProgress) {
  const fileHash = await calculateFileHash(file);
  const chunkSize = calculateChunkSize(file.size);
  const totalChunks = Math.ceil(file.size / chunkSize);

  // 创建上传控制器
  const controller = new TaskController(4);
  let uploadedCount = 0;

  // 不一次性创建所有切片
  for (let i = 0; i < totalChunks; i++) {
    const startPos = i * chunkSize;
    const endPos = Math.min(startPos + chunkSize, file.size);

    // 添加上传任务
    controller.addTask(async () => {
      // 创建切片
      const chunk = file.slice(startPos, endPos);

      // 上传切片
      await uploadSingleChunk({
        chunk,
        index: i,
        hash: fileHash,
        filename: file.name,
      });

      // 更新进度
      uploadedCount++;
      onProgress(uploadedCount / totalChunks);
    });
  }

  // 所有分片上传完成后，合并文件
  await mergeChunks(fileHash, file.name, totalChunks);
}
```

### 4. 网络异常处理

**难点**：网络条件不稳定时，上传容易中断，需要健壮的错误处理和重试机制

**解决方案**：

1. **指数退避算法**：失败重试时增加等待时间

```javascript
async function uploadWithRetry(fn, retries = 3, delay = 1000) {
  let attempts = 0;

  while (attempts < retries) {
    try {
      return await fn();
    } catch (err) {
      attempts++;
      if (attempts >= retries) {
        throw err;
      }

      // 指数退避：每次重试等待时间翻倍
      const backoffTime = delay * Math.pow(2, attempts - 1);
      // 添加随机抖动，避免多个请求同时重试
      const jitter = Math.random() * 1000;

      await new Promise((resolve) => setTimeout(resolve, backoffTime + jitter));
    }
  }
}

// 使用方式
async function uploadChunk(formData) {
  return uploadWithRetry(async () => {
    const response = await fetch("/api/upload/chunk", {
      method: "POST",
      body: formData,
    });

    if (!response.ok) {
      throw new Error("Chunk upload failed");
    }

    return await response.json();
  });
}
```

2. **网络状态监测**：监听网络状态变化，在网络恢复时自动继续上传

```javascript
function setupNetworkMonitor(resumeCallback) {
  // 监听网络状态
  window.addEventListener("online", () => {
    console.log("网络已恢复，重新开始上传");
    resumeCallback();
  });

  window.addEventListener("offline", () => {
    console.log("网络已断开，暂停上传");
    // 可以在这里暂停上传
  });
}
```

### 5. 服务器端存储与合并优化

**难点**：服务器需要高效地存储和管理大量的文件分片，并在上传完成后快速合并

**解决方案**：

1. **流式合并**：使用流式处理而不是一次性读取所有分片

```javascript
// 服务器端伪代码（Node.js）
async function mergeFileChunks(fileHash, totalChunks) {
  const chunksDir = path.resolve(UPLOAD_DIR, fileHash + "-chunks");
  const filePath = path.resolve(UPLOAD_DIR, fileHash);
  const writeStream = fs.createWriteStream(filePath);

  // 按顺序处理每个分片
  for (let i = 0; i < totalChunks; i++) {
    const chunkPath = path.resolve(chunksDir, i.toString());
    // 使用管道流式传输，避免一次性读取整个分片
    await new Promise((resolve, reject) => {
      const readStream = fs.createReadStream(chunkPath);
      readStream.pipe(writeStream, { end: false });
      readStream.on("end", resolve);
      readStream.on("error", reject);
    });
  }

  writeStream.end();
  return filePath;
}
```

2. **临时文件清理**：定期清理过期的临时分片文件

```javascript
// 服务器端伪代码
function cleanupExpiredChunks() {
  // 定期执行，如每天凌晨
  const EXPIRE_TIME = 24 * 60 * 60 * 1000; // 1天

  fs.readdir(UPLOAD_DIR, (err, files) => {
    if (err) return;

    files.forEach((file) => {
      if (file.includes("-chunks")) {
        const dirPath = path.resolve(UPLOAD_DIR, file);
        fs.stat(dirPath, (err, stats) => {
          if (err) return;

          // 检查最后修改时间
          const now = Date.now();
          if (now - stats.mtimeMs > EXPIRE_TIME) {
            // 删除过期目录
            fs.rm(dirPath, { recursive: true }, () => {});
          }
        });
      }
    });
  });
}

// 设置定时任务
setInterval(cleanupExpiredChunks, 12 * 60 * 60 * 1000); // 每12小时执行一次
```

## 六、用户体验优化

1. **上传进度展示**：提供详细的上传进度信息

```javascript
function updateProgress(file, progress) {
  // 进度展示
  const progressElement = document.getElementById("progress-" + file.id);
  progressElement.style.width = progress * 100 + "%";

  // 速度计算
  const now = Date.now();
  const elapsed = (now - file.startTime) / 1000;
  const loaded = progress * file.size;
  const speed = loaded / elapsed;

  // 预估剩余时间
  const remaining = (file.size - loaded) / speed;

  document.getElementById("speed-" + file.id).textContent = formatSpeed(speed);
  document.getElementById("remaining-" + file.id).textContent =
    formatTime(remaining);
}
```

2. **上传状态可视化**：显示每个分片的上传状态

```html
<div class="chunk-status">
  <div
    class="chunk-status-item"
    v-for="(chunk, index) in chunks"
    :key="index"
    :class="{'upload-success': chunk.uploaded, 'upload-error': chunk.error, 'uploading': chunk.uploading}"
  ></div>
</div>
```

3. **拖拽上传**：支持拖拽文件到上传区域

```javascript
function setupDragDrop() {
  const dropArea = document.getElementById("drop-area");

  // 阻止默认行为
  ["dragenter", "dragover", "dragleave", "drop"].forEach((eventName) => {
    dropArea.addEventListener(eventName, (e) => {
      e.preventDefault();
      e.stopPropagation();
    });
  });

  // 高亮显示拖拽区域
  ["dragenter", "dragover"].forEach((eventName) => {
    dropArea.addEventListener(eventName, () => {
      dropArea.classList.add("highlight");
    });
  });

  ["dragleave", "drop"].forEach((eventName) => {
    dropArea.addEventListener(eventName, () => {
      dropArea.classList.remove("highlight");
    });
  });

  // 处理拖拽文件
  dropArea.addEventListener("drop", (e) => {
    const files = e.dataTransfer.files;
    if (files.length > 0) {
      handleFileUpload(files[0]);
    }
  });
}
```

## 七、总结

实现前端大文件上传是一个综合性的技术挑战，涉及多个方面的优化和处理：

1. **核心技术点**：

   - 文件分片上传
   - 唯一标识（Hash）计算
   - 并发控制
   - 断点续传
   - 秒传功能

2. **主要难点**：

   - 文件 Hash 计算性能
   - 分片管理与并发控制
   - 网络异常处理
   - 服务器端存储与合并
   - 大文件内存占用

3. **通用解决方案**：
   - 使用 Web Worker 处理计算密集型任务
   - 动态调整分片大小和并发数
   - 实现健壮的重试机制
   - 采用流式处理减少内存使用
   - 前后端协同的断点续传机制

通过合理设计和优化，可以实现高效、可靠的大文件上传功能，大幅提升用户体验。
